{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58053409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from numba import njit\n",
    "from functools import lru_cache\n",
    "from IPython.display import Audio, display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import stops\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "imp.reload(stops)\n",
    "\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e29e2-e12c-4316-baf2-0304babc1213",
   "metadata": {},
   "source": [
    "# notes\n",
    "* we should check if we can move state between different lengths of sequences, if so we can train on many differnent bus lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10001ce-57c3-4339-93d9-567bc365b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_exception = \"http://www.wav-sounds.com/movie/austinpowers.wav\"\n",
    "url_exception = \"https://www.myinstants.com/media/sounds/roblox-death-sound_1.mp3\"\n",
    "\n",
    "\n",
    "def play_sound(self, etype, value, tb, tb_offset=None):\n",
    "    self.showtraceback((etype, value, tb), tb_offset=tb_offset)\n",
    "    display(Audio(url=url_exception, autoplay=True))\n",
    "\n",
    "\n",
    "get_ipython().set_custom_exc((Exception,), play_sound)\n",
    "\n",
    "url_done = (\n",
    "    \"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\"\n",
    ")\n",
    "url_done = \"https://www.myinstants.com/media/sounds/taco-bell-bong-sfx.mp3\"\n",
    "url_done = \"https://www.myinstants.com/media/sounds/magic_immune.mp3\"\n",
    "\n",
    "\n",
    "def allDone():\n",
    "    display(\n",
    "        Audio(\n",
    "            url=url_done,\n",
    "            autoplay=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003dcdc7-b55f-46a2-a2b3-dae2114eb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e6c388-83d1-460c-b6fe-aae45ed3ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d61a32-e857-4e07-b9bf-b9daf333bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(recompute, direction):\n",
    "    def df_pop(df, labels):\n",
    "        popped = df[labels]\n",
    "        df = df.drop(columns=labels)\n",
    "        return df, popped\n",
    "\n",
    "    data_dir = os.path.join(os.getcwd(), \"..\", \"data\")\n",
    "\n",
    "    trips, dates, labels = None, None, None\n",
    "    lstm_dir = os.path.join(data_dir, \"trips_for_lstm\")\n",
    "    files = [\n",
    "        f + \"_direction_\" + str(direction) + \".npy\"\n",
    "        for f in [\"non_categories\", \"categories\", \"labels\", \"cardinality\"]\n",
    "    ]\n",
    "\n",
    "    cardinality = []\n",
    "\n",
    "    if recompute:\n",
    "\n",
    "        pickle_path = os.path.join(data_dir, \"mega_pickle\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "        #     df = df.head(100000)\n",
    "\n",
    "        df = df.drop(\n",
    "            columns=[\"route_number\", \"time_cat_stop_time\", \"time_cat_leave_time\"]\n",
    "        )\n",
    "\n",
    "        df = df.sort_values([\"service_date\", \"train\", \"trip_number\", \"stop_time\"])\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        df = df[df[\"direction\"] == direction]\n",
    "\n",
    "        dtypes = df.dtypes\n",
    "        category_names = dtypes[dtypes == \"category\"]\n",
    "        category_names = category_names.index.to_list()\n",
    "        non_category_names = dtypes[dtypes != \"category\"]\n",
    "        non_category_names = non_category_names.index.to_list()\n",
    "        non_category_names.remove(\"service_date\")\n",
    "        non_category_names.remove(\"arrival_deviance\")\n",
    "        label_names = [\"arrival_deviance\"]\n",
    "\n",
    "        enc = OrdinalEncoder()\n",
    "        df[category_names] = enc.fit_transform(df[category_names])\n",
    "        df[category_names] = df[category_names].astype(np.int32)\n",
    "\n",
    "        for category in category_names:\n",
    "            cardinality.append(len(df[category].unique()))\n",
    "\n",
    "        trips = df.groupby([\"service_date\", \"train\", \"trip_number\"])\n",
    "        trips = [trip for _, trip in trips]\n",
    "\n",
    "        random.shuffle(\n",
    "            trips\n",
    "        )  # WE CAN DO THIS BECAUSE WE'RE ONLY TRYING TO PREDICT THE NEXT TRIP\n",
    "\n",
    "        hist = {}\n",
    "        for trip in trips:\n",
    "            trip = trip.drop_duplicates(\"location_id\")\n",
    "            s = tuple(trip[\"location_id\"].to_list())\n",
    "            if s in hist:\n",
    "                hist[s] = hist[s] + 1\n",
    "            else:\n",
    "                hist[s] = 1\n",
    "\n",
    "        sorted_stops = sorted(hist.items(), key=lambda x: x[1], reverse=True)\n",
    "        most_common_stop_sequence = list(sorted_stops[0][0])[\n",
    "            1:-1\n",
    "        ]  # LAST STOP WAS REGULARLY ~13 MINUTES EARLY...\n",
    "        cannonical_length = len(most_common_stop_sequence)\n",
    "\n",
    "        stop_order_dict = {}\n",
    "        for i, stop in enumerate(most_common_stop_sequence):\n",
    "            stop_order_dict[stop] = i\n",
    "\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        y = []\n",
    "        for trip in trips:\n",
    "            trip = trip.drop_duplicates(\"location_id\")\n",
    "\n",
    "            labels = trip[label_names]\n",
    "            categories = trip[category_names]\n",
    "            non_categories = trip[non_category_names]\n",
    "\n",
    "            cats = np.array(categories)\n",
    "            non_cats = np.array(non_categories)\n",
    "            l = np.array(labels)\n",
    "\n",
    "            c_cats = np.zeros((cannonical_length, cats.shape[1]))\n",
    "            c_non_cats = np.zeros((cannonical_length, non_cats.shape[1]))\n",
    "            c_l = np.zeros(cannonical_length)\n",
    "\n",
    "            loc_id_index = category_names.index(\"location_id\")\n",
    "\n",
    "            this_trips_stops = cats[:, loc_id_index].tolist()\n",
    "\n",
    "            for i, loc_id in enumerate(most_common_stop_sequence):\n",
    "                if loc_id in this_trips_stops:\n",
    "                    idx = this_trips_stops.index(loc_id)\n",
    "                    c_cats[i] = cats[idx]\n",
    "                    c_non_cats[i] = non_cats[idx]\n",
    "                    c_l[i] = l[idx]\n",
    "\n",
    "            x1.append(c_non_cats)\n",
    "            x2.append(c_cats)\n",
    "            y.append(c_l)\n",
    "\n",
    "        non_categories = np.stack(x1)\n",
    "        categories = np.stack(x2)\n",
    "        labels = np.stack(y)\n",
    "        cardinality = np.array(cardinality)\n",
    "\n",
    "        arrays = [non_categories, categories, labels, cardinality]\n",
    "\n",
    "        for file_name, array in zip(files, arrays):\n",
    "            file_path = os.path.join(lstm_dir, file_name)\n",
    "            np.save(file_path, array)\n",
    "\n",
    "    else:\n",
    "        arrays = []\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(lstm_dir, file_name)\n",
    "            arrays.append(np.load(file_path))\n",
    "        non_categories, categories, labels, cardinality = arrays\n",
    "\n",
    "    return non_categories, categories, labels, cardinality\n",
    "\n",
    "\n",
    "# non_categories, categories, labels, cardinality = get_data(direction=1, recompute=False)\n",
    "\n",
    "\n",
    "# print(\n",
    "#     non_categories.shape,\n",
    "#     categories.shape,\n",
    "#     labels.shape,\n",
    "# )\n",
    "\n",
    "\n",
    "# datalen = non_categories.shape[0]\n",
    "# num_stops = non_categories.shape[1]\n",
    "# non_category_width = non_categories.shape[2]\n",
    "# category_width = categories.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfaba6ea-fe38-4e2f-a84d-e82623908196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27521 13760 4588\n"
     ]
    }
   ],
   "source": [
    "class bus_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, recompute, direction):\n",
    "        #         self, annotations_file, img_dir, transform=None, target_transform=None\n",
    "        #     ):\n",
    "\n",
    "        self.non_categories, self.categories, self.labels, self.cardinality = get_data(\n",
    "            recompute, direction\n",
    "        )\n",
    "\n",
    "        self.datalen = self.non_categories.shape[0]\n",
    "        self.num_stops = self.non_categories.shape[1]\n",
    "        self.non_category_width = self.non_categories.shape[2]\n",
    "        self.category_width = self.categories.shape[2]\n",
    "        \n",
    "        self.non_categories_shape = (self.num_stops, self.non_category_width)\n",
    "        self.categories_shape = (self.num_stops, self.category_width)\n",
    "        self.label_shape = (self.num_stops,)\n",
    "\n",
    "\n",
    "    #         self.img_labels = pd.read_csv(annotations_file)\n",
    "    #         self.img_dir = img_dir\n",
    "    #         self.transform = transform\n",
    "    #         self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datalen\n",
    "\n",
    "    #         return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        non_category = self.non_categories[idx].copy()\n",
    "        category = self.categories[idx].copy()\n",
    "        label = self.labels[idx].copy()\n",
    "\n",
    "        r = random.randint(0, num_stops)\n",
    "        non_category[r:] = 0\n",
    "        category[r:] = 0\n",
    "\n",
    "        return (non_category, category, r), label\n",
    "\n",
    "\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label\n",
    "\n",
    "# get the dataset, pull datalen out of it (or just run len) and then you can use the random split thing...\n",
    "\n",
    "dataset = bus_dataset(recompute=False, direction=1)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train = 0.6\n",
    "val = 0.3\n",
    "test = 0.1\n",
    "assert round(train + val + test) == 1\n",
    "\n",
    "datalen = len(dataset)\n",
    "\n",
    "train = int(train * datalen)\n",
    "val = int(val * datalen)\n",
    "test = datalen - (train + val)\n",
    "\n",
    "print(train, val, test)\n",
    "\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [train + val, test])\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [train, val])\n",
    "\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_data = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train_dataset = dataset.take(train).shuffle(10000).batch(batch_size)\n",
    "# val_dataset = dataset.skip(train).take(val).batch(batch_size)\n",
    "# test_dataset = dataset.skip(train + val).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37fd4280-0531-43f8-9421-c15045d9c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_categories_shape = (num_stops, non_category_width)\n",
    "# categories_shape = (num_stops, category_width)\n",
    "# label_shape = (num_stops,)\n",
    "\n",
    "\n",
    "# num_samples = 4\n",
    "# datalen = datalen * num_samples\n",
    "# known_weight = 1\n",
    "\n",
    "\n",
    "# def data_generator(s):\n",
    "#     print(f\"working on {s}\")\n",
    "#     files = [\n",
    "#         f + \"_direction_\" + str(direction) + \".npy\"\n",
    "#         for f in [\"non_categories\", \"categories\", \"labels\", \"cardinality\"]\n",
    "#     ]\n",
    "#     arrays = []\n",
    "#     for file_name in files:\n",
    "#         file_path = os.path.join(lstm_dir, file_name)\n",
    "#         arrays.append(np.load(file_path))\n",
    "#     non_categories, categories, labels, cardinality = arrays\n",
    "\n",
    "#     for non_category, category, label in zip(non_categories, categories, labels):\n",
    "#         samples = random.sample(range(0, num_stops), num_samples)\n",
    "#         #         samples = sorted(random_list, reverse=True)\n",
    "#         for i in samples:\n",
    "#             nc = non_category.copy()\n",
    "#             c = category.copy()\n",
    "#             l = label.copy()\n",
    "#             #             mask = np.ones_like(l)\n",
    "#             nc[i:] = 0\n",
    "#             c[i:] = 0\n",
    "#             #             mask[:i] *= known_weight\n",
    "#             l[:i] *= known_weight\n",
    "#             yield {\n",
    "#                 \"non_categories\": nc,\n",
    "#                 \"categories\": c,\n",
    "#                 \"num_samples\": i,\n",
    "#                 #                 \"mask\": mask,\n",
    "#             }, l\n",
    "\n",
    "\n",
    "# def get_dataset(non_categories, categories, labels):\n",
    "#     return tf.data.Dataset.from_generator(\n",
    "#         data_generator,\n",
    "#         args=[non_categories, categories, labels],\n",
    "#         output_signature=(\n",
    "#             {\n",
    "#                 \"non_categories\": tf.TensorSpec(\n",
    "#                     shape=non_categories_shape, dtype=tf.float32\n",
    "#                 ),\n",
    "#                 \"categories\": tf.TensorSpec(shape=categories_shape, dtype=tf.int32),\n",
    "#                 \"num_samples\": tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "#                 #                 \"mask\": tf.TensorSpec(shape=label_shape, dtype=tf.int32),\n",
    "#             },\n",
    "#             tf.TensorSpec(shape=label_shape, dtype=tf.float32),\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "# Dataset = tf.data.Dataset\n",
    "# dataset = Dataset.from_tensor_slices([\"Gen_0\", \"Gen_1\", \"Gen_2\"])\n",
    "# dataset = ds.interleave(\n",
    "#     lambda x: Dataset.from_generator(\n",
    "#         data_generator,\n",
    "#         args=(x,),\n",
    "#         output_signature=(\n",
    "#             {\n",
    "#                 \"non_categories\": tf.TensorSpec(\n",
    "#                     shape=non_categories_shape, dtype=tf.float32\n",
    "#                 ),\n",
    "#                 \"categories\": tf.TensorSpec(shape=categories_shape, dtype=tf.int32),\n",
    "#                 \"num_samples\": tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "#                 #                 \"mask\": tf.TensorSpec(shape=label_shape, dtype=tf.int32),\n",
    "#             },\n",
    "#             tf.TensorSpec(shape=label_shape, dtype=tf.float32),\n",
    "#         ),\n",
    "#     ),\n",
    "#     cycle_length=3,\n",
    "#     block_length=1,\n",
    "#     num_parallel_calls=3,\n",
    "# )\n",
    "\n",
    "# Dataset = tf.data.Dataset\n",
    "# ds = Dataset.from_tensor_slices(['Gen_0', 'Gen_1', 'Gen_2'])\n",
    "# ds = ds.interleave(lambda x: Dataset.from_generator(py_gen, output_types=(tf.string), args=(x,)),\n",
    "#                    cycle_length=3,\n",
    "#                    block_length=1,\n",
    "#                    num_parallel_calls=3)\n",
    "# dataset = ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75e1e3-db50-4ebd-b14c-e8aeecc1c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# train = 0.6\n",
    "# val = 0.4\n",
    "# test = 0.0\n",
    "# assert round(train + val + test) == 1\n",
    "\n",
    "# train = int(train * datalen)\n",
    "# val = int(val * datalen)\n",
    "# test = int(test * datalen)\n",
    "\n",
    "# print(train, val, test)\n",
    "\n",
    "# # dataset = get_dataset(non_categories, categories, labels)\n",
    "\n",
    "# train_dataset = dataset.take(train).shuffle(10000).batch(batch_size)\n",
    "# val_dataset = dataset.skip(train).take(val).batch(batch_size)\n",
    "# test_dataset = dataset.skip(train + val).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1690e4c-e779-4d1c-8f84-4cff12b23e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac05b9-87e0-42c9-9441-939ae28357cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.2\n",
    "lstm_size = 32\n",
    "\n",
    "# https://stackoverflow.com/questions/56729139/faster-way-to-do-multiple-embeddings-in-pytorch\n",
    "\n",
    "non_categories_input = tf.keras.Input(shape=non_categories_shape, name=\"non_categories\")\n",
    "categories_input = tf.keras.Input(shape=categories_shape, name=\"categories\")\n",
    "# mask_input = tf.keras.Input(shape=label_shape, name=\"mask\")\n",
    "\n",
    "non_categories = non_categories_input\n",
    "categories = categories_input\n",
    "# mask = mask_input\n",
    "\n",
    "cat_list = tf.unstack(categories, axis=2)\n",
    "embed_out = []\n",
    "for cat, card in zip(cat_list, cardinality):\n",
    "    embed_out.append(tf.keras.layers.Embedding(card, card // 3 + 1)(cat))\n",
    "\n",
    "lstm_prep = tf.keras.layers.Concatenate(axis=2)([non_categories, *embed_out])\n",
    "\n",
    "concat_width = lstm_prep.shape[2]\n",
    "\n",
    "lstm_prep = tf.keras.layers.Dense((lstm_size + concat_width) // 2 + 1)(lstm_prep)\n",
    "lstm_prep = tf.keras.layers.Dense(lstm_size)(lstm_prep)\n",
    "block_out = tf.keras.layers.Reshape((num_stops, lstm_size))(lstm_prep)\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "\n",
    "out_funnel = tf.keras.layers.Dense(1)\n",
    "\n",
    "pred = tf.keras.layers.TimeDistributed(out_funnel)(lstm_layers)\n",
    "\n",
    "pred = tf.keras.layers.Reshape(label_shape)(pred)\n",
    "# pred = pred * mask\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[non_categories_input, categories],\n",
    "    outputs=[pred],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00907bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
    "    metrics=[\"MAE\"],\n",
    ")\n",
    "print(\"total parameters: {:,}\".format(model.count_params()))\n",
    "tf.keras.utils.plot_model(model, \"model.png\", show_shapes=True, dpi=227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9e9dc1-7619-47c1-9557-e2c432f715be",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_param = \"val_loss\"\n",
    "\n",
    "nan_terminate = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_param, patience=10  # , restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_dir = os.path.join(data_dir, \"model\")\n",
    "checkpoint_filepath = os.path.join(model_dir, \"checkpoint\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=monitor_param,\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5898bc1c-6644-4f0a-a508-01d32e567636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\graham\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\keras\\engine\\functional.py:582: UserWarning: Input dict contained keys ['num_samples'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3441/3441 [==============================] - 356s 99ms/step - loss: 26170.1562 - MAE: 85.5732 - val_loss: 26518.2090 - val_MAE: 86.4914\n",
      "Epoch 2/100\n",
      "3441/3441 [==============================] - ETA: 0s - loss: 25890.3281 - MAE: 85.9101"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16784/744782589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1215\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1216\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1501\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bus-prediction-iVe0Nv0y-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, checkpoint, nan_terminate],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268ccfe-a57b-4771-9698-13a94795c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7420fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aef57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f46bd-ebc7-40a2-99e3-7ca401003981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39125f-7377-4962-8764-0d9fe064ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (pred, y) in enumerate(zip(preds, test_dataset.unbatch().as_numpy_iterator())):\n",
    "    if i < 100:\n",
    "        x, y = y\n",
    "        pred_len = pred.shape[0]\n",
    "\n",
    "        num_samples = x[\"num_samples\"]\n",
    "        samp = [0] * num_samples\n",
    "\n",
    "        x = list(range(pred_len))\n",
    "        #         print(pred)\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = [20, 4]\n",
    "        plt.scatter(x, pred, label=\"pred\")\n",
    "        plt.scatter(x, y, label=\"y\")\n",
    "        plt.scatter(list(range(num_samples)), samp, label=\"samp\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aeeba3-68f4-4e2c-abe7-843ba2b3fa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus_pred",
   "language": "python",
   "name": "bus_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
