{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58053409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import tensorflow as tf\n",
    "import pickle\n",
    "from numba import njit\n",
    "from functools import lru_cache\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import random\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "import stops\n",
    "import make_loaders\n",
    "\n",
    "# tf.random.set_seed(1234)\n",
    "\n",
    "imp.reload(stops)\n",
    "imp.reload(make_loaders)\n",
    "\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e29e2-e12c-4316-baf2-0304babc1213",
   "metadata": {},
   "source": [
    "# notes\n",
    "* we should check if we can move state between different lengths of sequences, if so we can train on many differnent bus lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f10001ce-57c3-4339-93d9-567bc365b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_exception = \"http://www.wav-sounds.com/movie/austinpowers.wav\"\n",
    "url_exception = \"https://www.myinstants.com/media/sounds/roblox-death-sound_1.mp3\"\n",
    "\n",
    "\n",
    "def play_sound(self, etype, value, tb, tb_offset=None):\n",
    "    self.showtraceback((etype, value, tb), tb_offset=tb_offset)\n",
    "    display(Audio(url=url_exception, autoplay=True))\n",
    "\n",
    "\n",
    "get_ipython().set_custom_exc((Exception,), play_sound)\n",
    "\n",
    "url_done = (\n",
    "    \"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\"\n",
    ")\n",
    "url_done = \"https://www.myinstants.com/media/sounds/taco-bell-bong-sfx.mp3\"\n",
    "url_done = \"https://www.myinstants.com/media/sounds/magic_immune.mp3\"\n",
    "\n",
    "\n",
    "def allDone():\n",
    "    display(\n",
    "        Audio(\n",
    "            url=url_done,\n",
    "            autoplay=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "003dcdc7-b55f-46a2-a2b3-dae2114eb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18e6c388-83d1-460c-b6fe-aae45ed3ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d61a32-e857-4e07-b9bf-b9daf333bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(recompute, direction):\n",
    "    def df_pop(df, labels):\n",
    "        popped = df[labels]\n",
    "        df = df.drop(columns=labels)\n",
    "        return df, popped\n",
    "\n",
    "    data_dir = os.path.join(os.getcwd(), \"..\", \"data\")\n",
    "\n",
    "    trips, dates, labels = None, None, None\n",
    "    lstm_dir = os.path.join(data_dir, \"trips_for_lstm\")\n",
    "    files = [\n",
    "        f\"{f}_direction_{direction}.npy\"\n",
    "        for f in [\"non_categories\", \"categories\", \"labels\", \"cardinality\"]\n",
    "    ]\n",
    "\n",
    "    cardinality = []\n",
    "\n",
    "    if recompute:\n",
    "\n",
    "        pickle_path = os.path.join(data_dir, \"mega_pickle\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "        #     df = df.head(100000)\n",
    "\n",
    "        df = df.drop(\n",
    "            columns=[\"route_number\", \"time_cat_stop_time\", \"time_cat_leave_time\"]\n",
    "        )\n",
    "\n",
    "        df = df.sort_values([\"service_date\", \"train\", \"trip_number\", \"stop_time\"])\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        df = df[df[\"direction\"] == direction]\n",
    "\n",
    "        dtypes = df.dtypes\n",
    "        category_names = dtypes[dtypes == \"category\"]\n",
    "        category_names = category_names.index.to_list()\n",
    "        non_category_names = dtypes[dtypes != \"category\"]\n",
    "        non_category_names = non_category_names.index.to_list()\n",
    "        non_category_names.remove(\"service_date\")\n",
    "        non_category_names.remove(\"arrival_deviance\")\n",
    "        label_names = [\"arrival_deviance\"]\n",
    "\n",
    "        enc = OrdinalEncoder()\n",
    "        df[category_names] = enc.fit_transform(df[category_names])\n",
    "        df[category_names] = df[category_names].astype(np.int32)\n",
    "\n",
    "        for category in category_names:\n",
    "            cardinality.append(len(df[category].unique()))\n",
    "\n",
    "        trips = df.groupby([\"service_date\", \"train\", \"trip_number\"])\n",
    "        trips = [trip for _, trip in trips]\n",
    "\n",
    "        random.shuffle(\n",
    "            trips\n",
    "        )  # WE CAN DO THIS BECAUSE WE'RE ONLY TRYING TO PREDICT THE NEXT TRIP\n",
    "\n",
    "        hist = {}\n",
    "        for trip in trips:\n",
    "            trip = trip.drop_duplicates(\"location_id\")\n",
    "            s = tuple(trip[\"location_id\"].to_list())\n",
    "            if s in hist:\n",
    "                hist[s] = hist[s] + 1\n",
    "            else:\n",
    "                hist[s] = 1\n",
    "\n",
    "        sorted_stops = sorted(hist.items(), key=lambda x: x[1], reverse=True)\n",
    "        most_common_stop_sequence = list(sorted_stops[0][0])[\n",
    "            1:-1\n",
    "        ]  # LAST STOP WAS REGULARLY ~13 MINUTES EARLY...\n",
    "        cannonical_length = len(most_common_stop_sequence)\n",
    "\n",
    "        stop_order_dict = {}\n",
    "        for i, stop in enumerate(most_common_stop_sequence):\n",
    "            stop_order_dict[stop] = i\n",
    "\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        y = []\n",
    "        for trip in trips:\n",
    "            trip = trip.drop_duplicates(\"location_id\")\n",
    "\n",
    "            labels = trip[label_names]\n",
    "            categories = trip[category_names]\n",
    "            non_categories = trip[non_category_names]\n",
    "\n",
    "            cats = np.array(categories)\n",
    "            non_cats = np.array(non_categories)\n",
    "            l = np.array(labels)\n",
    "\n",
    "            c_cats = np.zeros((cannonical_length, cats.shape[1]))\n",
    "            c_non_cats = np.zeros((cannonical_length, non_cats.shape[1]))\n",
    "            c_l = np.zeros(cannonical_length)\n",
    "\n",
    "            loc_id_index = category_names.index(\"location_id\")\n",
    "\n",
    "            this_trips_stops = cats[:, loc_id_index].tolist()\n",
    "\n",
    "            for i, loc_id in enumerate(most_common_stop_sequence):\n",
    "                if loc_id in this_trips_stops:\n",
    "                    idx = this_trips_stops.index(loc_id)\n",
    "                    c_cats[i] = cats[idx]\n",
    "                    c_non_cats[i] = non_cats[idx]\n",
    "                    c_l[i] = l[idx]\n",
    "\n",
    "            x1.append(c_non_cats)\n",
    "            x2.append(c_cats)\n",
    "            y.append(c_l)\n",
    "\n",
    "        non_categories = np.stack(x1)\n",
    "        categories = np.stack(x2)\n",
    "        labels = np.stack(y)\n",
    "        cardinality = np.array(cardinality)\n",
    "\n",
    "        arrays = [non_categories, categories, labels, cardinality]\n",
    "\n",
    "        for file_name, array in zip(files, arrays):\n",
    "            file_path = os.path.join(lstm_dir, file_name)\n",
    "            np.save(file_path, array)\n",
    "\n",
    "    else:\n",
    "        arrays = []\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(lstm_dir, file_name)\n",
    "            arrays.append(np.load(file_path))\n",
    "        non_categories, categories, labels, cardinality = arrays\n",
    "\n",
    "    return non_categories, categories, labels, cardinality\n",
    "\n",
    "\n",
    "# non_categories, categories, labels, cardinality = get_data(direction=1, recompute=False)\n",
    "\n",
    "\n",
    "# print(\n",
    "#     non_categories.shape,\n",
    "#     categories.shape,\n",
    "#     labels.shape,\n",
    "# )\n",
    "\n",
    "\n",
    "# datalen = non_categories.shape[0]\n",
    "# num_stops = non_categories.shape[1]\n",
    "# non_category_width = non_categories.shape[2]\n",
    "# category_width = categories.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfaba6ea-fe38-4e2f-a84d-e82623908196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27521 13760 4588\n",
      "made dataloaders\n"
     ]
    }
   ],
   "source": [
    "class bus_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, recompute, direction, batch_size):\n",
    "\n",
    "        self.non_categories, self.categories, self.labels, self.cardinality = get_data(\n",
    "            recompute, direction\n",
    "        )\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.datalen = self.non_categories.shape[0]\n",
    "        self.num_stops = self.non_categories.shape[1]\n",
    "        self.non_category_width = self.non_categories.shape[2]\n",
    "        self.category_width = self.categories.shape[2]\n",
    "\n",
    "        self.non_categories_shape = (self.num_stops, self.non_category_width)\n",
    "        self.categories_shape = (self.num_stops, self.category_width)\n",
    "        self.label_shape = (self.num_stops,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datalen\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r_int = random.randint(0, self.num_stops)\n",
    "        r = torch.tensor(r_int).float()\n",
    "        non_category = torch.tensor(self.non_categories[idx]).float()\n",
    "        category = torch.tensor(self.categories[idx]).float()\n",
    "        label = torch.tensor([self.labels[idx]]).float()\n",
    "\n",
    "        # MIGHT OVERWRITE MEMORY WITH ZEROS, CHECK THAT TORCH.TENSOR MAKES NEW MEMORY\n",
    "        #         r_unpacked = r[0]\n",
    "        #         print(r_unpacked)\n",
    "\n",
    "        non_category[r_int:] = 0\n",
    "        category[r_int:] = 0\n",
    "\n",
    "        label = torch.transpose(label, 0, 1)\n",
    "\n",
    "        #         print('label', label.shape)\n",
    "        #         print('non_category', non_category.shape)\n",
    "        #         print('category', category.shape)\n",
    "\n",
    "        return (non_category, category, r), label\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = bus_dataset(recompute=False, direction=1, batch_size=batch_size)\n",
    "\n",
    "train = 0.6\n",
    "val = 0.3\n",
    "test = 0.1\n",
    "assert round(train + val + test) == 1\n",
    "\n",
    "datalen = len(dataset)\n",
    "\n",
    "train = int(train * datalen)\n",
    "val = int(val * datalen)\n",
    "test = datalen - (train + val)\n",
    "\n",
    "print(train, val, test)\n",
    "\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [train + val, test])\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [train, val])\n",
    "\n",
    "\n",
    "train_loader, val_loaders, test_loader = make_loaders.make_loaders(\n",
    "    train_data, val_data, test_data, batch_size\n",
    ")\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True,\n",
    "#         num_workers=12,\n",
    "#     )\n",
    "#     val_loader = torch.utils.data.DataLoader(\n",
    "#         val_data,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=12,\n",
    "#     )\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_data,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=12,\n",
    "#     )\n",
    "\n",
    "print(\"made dataloaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1690e4c-e779-4d1c-8f84-4cff12b23e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.2\n",
    "lstm_size = 32\n",
    "\n",
    "\n",
    "class LSTM(pytorch_lightning.core.lightning.LightningModule):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"init model\")\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.lstm_1 = torch.nn.LSTM(\n",
    "            self.data.non_category_width, lstm_size, batch_first=True, num_layers=3\n",
    "        )\n",
    "        self.lstm_to_outs = torch.nn.Linear(lstm_size, 1)\n",
    "\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/discussions/6748#discussioncomment-568785\n",
    "\n",
    "    def forward(self, x):\n",
    "        non_category, category, r = x\n",
    "        x, state = self.lstm_1(non_category)\n",
    "        x = self.lstm_to_outs(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(\"configure opt\")\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56617b6b-e23a-4e23-9389-4a08d9ba00cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model\n",
      "made trainer\n",
      "configure opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type   | Params\n",
      "----------------------------------------\n",
      "0 | lstm_1       | LSTM   | 23.4 K\n",
      "1 | lstm_to_outs | Linear | 33    \n",
      "----------------------------------------\n",
      "23.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 K    Total params\n",
      "0.094     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                                                  | 0/861 [00:00<00:08, 98.14it/s]"
     ]
    }
   ],
   "source": [
    "model = LSTM(data=dataset)\n",
    "trainer = pytorch_lightning.Trainer(gpus=1)\n",
    "print(\"made trainer\")\n",
    "trainer.fit(model, train_loader)\n",
    "print(\"done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac05b9-87e0-42c9-9441-939ae28357cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.2\n",
    "lstm_size = 32\n",
    "\n",
    "# https://stackoverflow.com/questions/56729139/faster-way-to-do-multiple-embeddings-in-pytorch\n",
    "\n",
    "non_categories_input = tf.keras.Input(shape=non_categories_shape, name=\"non_categories\")\n",
    "categories_input = tf.keras.Input(shape=categories_shape, name=\"categories\")\n",
    "# mask_input = tf.keras.Input(shape=label_shape, name=\"mask\")\n",
    "\n",
    "non_categories = non_categories_input\n",
    "categories = categories_input\n",
    "# mask = mask_input\n",
    "\n",
    "cat_list = tf.unstack(categories, axis=2)\n",
    "embed_out = []\n",
    "for cat, card in zip(cat_list, cardinality):\n",
    "    embed_out.append(tf.keras.layers.Embedding(card, card // 3 + 1)(cat))\n",
    "\n",
    "lstm_prep = tf.keras.layers.Concatenate(axis=2)([non_categories, *embed_out])\n",
    "\n",
    "concat_width = lstm_prep.shape[2]\n",
    "\n",
    "lstm_prep = tf.keras.layers.Dense((lstm_size + concat_width) // 2 + 1)(lstm_prep)\n",
    "lstm_prep = tf.keras.layers.Dense(lstm_size)(lstm_prep)\n",
    "block_out = tf.keras.layers.Reshape((num_stops, lstm_size))(lstm_prep)\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "lstm_layers = tf.keras.layers.LSTM(lstm_size, return_sequences=True)(lstm_layers)\n",
    "\n",
    "block_out = tf.keras.layers.add([block_out, lstm_layers])\n",
    "\n",
    "\n",
    "lstm_layers = tf.keras.layers.BatchNormalization()(block_out)\n",
    "lstm_layers = tf.keras.layers.Dropout(rate=drop_rate)(lstm_layers)\n",
    "\n",
    "out_funnel = tf.keras.layers.Dense(1)\n",
    "\n",
    "pred = tf.keras.layers.TimeDistributed(out_funnel)(lstm_layers)\n",
    "\n",
    "pred = tf.keras.layers.Reshape(label_shape)(pred)\n",
    "# pred = pred * mask\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[non_categories_input, categories],\n",
    "    outputs=[pred],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00907bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
    "    metrics=[\"MAE\"],\n",
    ")\n",
    "print(\"total parameters: {:,}\".format(model.count_params()))\n",
    "tf.keras.utils.plot_model(model, \"model.png\", show_shapes=True, dpi=227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e9dc1-7619-47c1-9557-e2c432f715be",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_param = \"val_loss\"\n",
    "\n",
    "nan_terminate = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_param, patience=10  # , restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_dir = os.path.join(data_dir, \"model\")\n",
    "checkpoint_filepath = os.path.join(model_dir, \"checkpoint\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=monitor_param,\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898bc1c-6644-4f0a-a508-01d32e567636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, checkpoint, nan_terminate],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268ccfe-a57b-4771-9698-13a94795c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7420fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aef57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f46bd-ebc7-40a2-99e3-7ca401003981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39125f-7377-4962-8764-0d9fe064ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (pred, y) in enumerate(zip(preds, test_dataset.unbatch().as_numpy_iterator())):\n",
    "    if i < 100:\n",
    "        x, y = y\n",
    "        pred_len = pred.shape[0]\n",
    "\n",
    "        num_samples = x[\"num_samples\"]\n",
    "        samp = [0] * num_samples\n",
    "\n",
    "        x = list(range(pred_len))\n",
    "        #         print(pred)\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = [20, 4]\n",
    "        plt.scatter(x, pred, label=\"pred\")\n",
    "        plt.scatter(x, y, label=\"y\")\n",
    "        plt.scatter(list(range(num_samples)), samp, label=\"samp\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aeeba3-68f4-4e2c-abe7-843ba2b3fa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus_pred",
   "language": "python",
   "name": "bus_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
